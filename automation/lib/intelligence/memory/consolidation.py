"""
MH1 Memory Consolidation Manager

Orchestrates the memory lifecycle: Working → Episodic → Semantic → Procedural

This module handles the automatic promotion and decay of memories across layers:
1. Working memory (volatile) → Episodic memory (upon outcome observation)
2. Episodic memory (decaying) → Semantic patterns (upon consolidation threshold)
3. Semantic patterns (with confidence) → Procedural knowledge (cross-skill patterns)

Firebase paths:
- Episodic: system/intelligence/episodic/{tenant_id}/{skill_name}/{episode_id}
- Semantic: system/intelligence/semantic/{tenant_id}/{skill_name}/{pattern_id}
- Procedural: system/intelligence/procedural/{pattern_id}
"""

from __future__ import annotations

import hashlib
import json
import logging
import threading
from collections import defaultdict
from dataclasses import dataclass, field
from statistics import mean, mode, StatisticsError
from typing import Any, Dict, List, Optional, TYPE_CHECKING

from .episodic import EpisodicMemoryStore
from ..types import Domain, SemanticPattern

# These imports will work once the stores are implemented
# Using TYPE_CHECKING to allow forward references for type hints
if TYPE_CHECKING:
    from .semantic import SemanticMemoryStore
    from .procedural import ProceduralMemoryStore, ProceduralKnowledge

logger = logging.getLogger(__name__)


@dataclass
class ConsolidationConfig:
    """Configuration for memory consolidation behavior."""
    consolidation_batch_size: int = 20          # Max episodes to consolidate per skill per cycle
    min_episodes_for_consolidation: int = 5     # Min episodes needed before consolidation
    cross_skill_threshold: int = 3              # Skills needed for procedural knowledge
    cross_skill_min_confidence: float = 0.6     # Min avg confidence for cross-skill patterns


class MemoryConsolidationManager:
    """
    Orchestrates memory consolidation across all memory layers.
    
    The consolidation cycle:
    1. Apply temporal decay to all episodic memories
    2. Promote decayed episodes to semantic patterns
    3. Archive stale semantic patterns
    4. Promote cross-skill patterns to procedural knowledge
    
    Thread Safety:
        All operations are protected by an RLock to ensure thread-safe access
        in multi-threaded environments.
    
    Example:
        >>> consolidation = MemoryConsolidationManager(
        ...     episodic_store=episodic,
        ...     semantic_store=semantic,
        ...     procedural_store=procedural,
        ... )
        >>> stats = consolidation.run_consolidation_cycle(tenant_id="acme-corp")
        >>> print(f"Consolidated {stats['episodes_consolidated']} episodes")
    """
    
    def __init__(
        self,
        episodic_store: EpisodicMemoryStore,
        semantic_store: "SemanticMemoryStore",
        procedural_store: "ProceduralMemoryStore",
        config: Optional[ConsolidationConfig] = None
    ):
        """
        Initialize the consolidation manager.
        
        Args:
            episodic_store: Store for episodic memories
            semantic_store: Store for semantic patterns
            procedural_store: Store for procedural knowledge
            config: Configuration for consolidation thresholds
        """
        self._episodic = episodic_store
        self._semantic = semantic_store
        self._procedural = procedural_store
        self._config = config or ConsolidationConfig()
        self._lock = threading.RLock()
    
    def run_consolidation_cycle(
        self,
        tenant_id: Optional[str] = None
    ) -> Dict[str, int]:
        """
        Run a full memory consolidation cycle.
        
        This is the main orchestration method that should be called periodically
        (e.g., via a cron job or background task).
        
        Args:
            tenant_id: Optional tenant to consolidate. If None, processes all tenants.
        
        Returns:
            Statistics dict with keys:
            - episodic_decayed: Number of episodes that had decay applied
            - episodes_consolidated: Number of episodes promoted to semantic
            - patterns_created: Number of new semantic patterns created
            - patterns_updated: Number of existing patterns updated
            - patterns_archived: Number of stale patterns archived
            - procedural_created: Number of new procedural knowledge entries
        """
        with self._lock:
            stats = {
                "episodic_decayed": 0,
                "episodes_consolidated": 0,
                "patterns_created": 0,
                "patterns_updated": 0,
                "patterns_archived": 0,
                "procedural_created": 0,
            }
            
            logger.info(f"Starting consolidation cycle for tenant: {tenant_id or 'all'}")
            
            try:
                # Step 1: Apply temporal decay to episodic memories
                logger.debug("Step 1: Applying temporal decay to episodic memories")
                decay_stats = self._episodic.decay_all(tenant_id)
                stats["episodic_decayed"] = decay_stats.get("decayed", 0)
                logger.info(f"Decayed {stats['episodic_decayed']} episodic memories")
                
                # Step 2: Consolidate ready episodes to semantic patterns
                logger.debug("Step 2: Consolidating ready episodes to semantic patterns")
                consolidation_stats = self._consolidate_ready_episodes(tenant_id)
                stats["episodes_consolidated"] = consolidation_stats.get("episodes_consolidated", 0)
                stats["patterns_created"] = consolidation_stats.get("patterns_created", 0)
                stats["patterns_updated"] = consolidation_stats.get("patterns_updated", 0)
                logger.info(
                    f"Consolidated {stats['episodes_consolidated']} episodes, "
                    f"created {stats['patterns_created']} patterns, "
                    f"updated {stats['patterns_updated']} patterns"
                )
                
                # Step 3: Archive stale semantic patterns
                logger.debug("Step 3: Archiving stale semantic patterns")
                if hasattr(self._semantic, 'forget_stale_patterns'):
                    archived_count = self._semantic.forget_stale_patterns()
                    stats["patterns_archived"] = archived_count
                    logger.info(f"Archived {stats['patterns_archived']} stale patterns")
                else:
                    logger.warning("Semantic store missing forget_stale_patterns method")
                
                # Step 4: Promote cross-skill patterns to procedural knowledge
                logger.debug("Step 4: Promoting cross-skill patterns to procedural knowledge")
                procedural_count = self._promote_to_procedural()
                stats["procedural_created"] = procedural_count
                logger.info(f"Created {stats['procedural_created']} procedural knowledge entries")
                
            except Exception as e:
                logger.error(f"Error during consolidation cycle: {e}", exc_info=True)
            
            logger.info(f"Consolidation cycle complete: {stats}")
            return stats
    
    def _consolidate_ready_episodes(
        self,
        tenant_id: Optional[str] = None
    ) -> Dict[str, int]:
        """
        Consolidate episodes ready for promotion to semantic patterns.
        
        Episodes are ready when their weight drops below the relevance threshold.
        
        Args:
            tenant_id: Optional tenant filter. If None, processes all tenants.
        
        Returns:
            Statistics dict with keys:
            - episodes_consolidated: Number of episodes processed
            - patterns_created: Number of new patterns created
            - patterns_updated: Number of existing patterns updated
        """
        stats = {
            "episodes_consolidated": 0,
            "patterns_created": 0,
            "patterns_updated": 0,
        }
        
        try:
            # Get tenants to process
            if tenant_id:
                tenants = [tenant_id]
            else:
                tenants = self._get_all_tenants()
            
            if not tenants:
                logger.debug("No tenants found for consolidation")
                return stats
            
            for tid in tenants:
                skills = self._get_skills_for_tenant(tid)
                
                for skill_name in skills:
                    # Get episodes ready for consolidation
                    ready_episodes = self._episodic.get_for_consolidation(
                        tenant_id=tid,
                        skill_name=skill_name,
                        limit=self._config.consolidation_batch_size
                    )
                    
                    if len(ready_episodes) < self._config.min_episodes_for_consolidation:
                        logger.debug(
                            f"Insufficient episodes for {tid}/{skill_name}: "
                            f"{len(ready_episodes)} < {self._config.min_episodes_for_consolidation}"
                        )
                        continue
                    
                    # Consolidate episodes into semantic patterns
                    if hasattr(self._semantic, 'consolidate_episodes'):
                        result = self._semantic.consolidate_episodes(
                            tenant_id=tid,
                            skill_name=skill_name,
                            episodes=ready_episodes
                        )
                        stats["patterns_created"] += result.get("created", 0)
                        stats["patterns_updated"] += result.get("updated", 0)
                    else:
                        logger.warning("Semantic store missing consolidate_episodes method")
                        continue
                    
                    # Mark episodes as consolidated
                    for episode in ready_episodes:
                        self._episodic.mark_consolidated(
                            episode_id=episode.episode_id,
                            tenant_id=tid,
                            skill_name=skill_name
                        )
                        stats["episodes_consolidated"] += 1
                    
                    logger.debug(
                        f"Consolidated {len(ready_episodes)} episodes for {tid}/{skill_name}"
                    )
                    
        except Exception as e:
            logger.error(f"Error in _consolidate_ready_episodes: {e}", exc_info=True)
        
        return stats
    
    def _promote_to_procedural(self) -> int:
        """
        Promote high-confidence cross-skill patterns to procedural knowledge.
        
        Finds patterns that appear across multiple skills with high confidence
        and creates procedural knowledge entries that apply universally.
        
        Returns:
            Number of procedural knowledge entries created
        """
        created_count = 0
        
        try:
            # Find cross-skill pattern groups
            pattern_groups = self._find_cross_skill_patterns()
            
            for group in pattern_groups:
                # Check if meets threshold requirements
                skills = group.get("skills", [])
                patterns = group.get("patterns", [])
                
                if len(skills) < self._config.cross_skill_threshold:
                    continue
                
                # Calculate average confidence
                if patterns:
                    avg_confidence = mean(p.confidence for p in patterns)
                else:
                    avg_confidence = 0.0
                
                if avg_confidence < self._config.cross_skill_min_confidence:
                    logger.debug(
                        f"Cross-skill pattern group below confidence threshold: "
                        f"{avg_confidence:.2f} < {self._config.cross_skill_min_confidence}"
                    )
                    continue
                
                # Generate procedural knowledge
                if hasattr(self._procedural, 'create_from_patterns'):
                    description = self._generate_description(group)
                    merged_recommendation = self._merge_recommendations(patterns)
                    
                    result = self._procedural.create_from_patterns(
                        condition_key=group.get("condition_key", ""),
                        skills=skills,
                        patterns=patterns,
                        recommendation=merged_recommendation,
                        description=description,
                        confidence=avg_confidence
                    )
                    
                    if result:
                        created_count += 1
                        logger.info(
                            f"Created procedural knowledge from {len(patterns)} patterns "
                            f"across {len(skills)} skills"
                        )
                else:
                    logger.warning("Procedural store missing create_from_patterns method")
                    
        except Exception as e:
            logger.error(f"Error in _promote_to_procedural: {e}", exc_info=True)
        
        return created_count
    
    def _find_cross_skill_patterns(self) -> List[Dict[str, Any]]:
        """
        Find patterns that appear across multiple skills.
        
        Groups patterns by similar conditions to identify cross-skill knowledge.
        
        Returns:
            List of pattern groups, each with format:
            {
                "condition_key": str,  # Hashable key for the condition
                "skills": List[str],   # Skills that have this pattern
                "patterns": List[SemanticPattern],  # The matching patterns
                "common_recommendation": Dict  # Merged recommendations
            }
        """
        pattern_groups: Dict[str, Dict[str, Any]] = defaultdict(
            lambda: {"skills": set(), "patterns": [], "condition_key": ""}
        )
        
        try:
            # Get all high-confidence patterns across all domains
            if hasattr(self._semantic, 'get_high_confidence_patterns'):
                patterns = self._semantic.get_high_confidence_patterns(
                    min_confidence=self._config.cross_skill_min_confidence
                )
            elif hasattr(self._semantic, 'get_all_patterns'):
                all_patterns = self._semantic.get_all_patterns()
                patterns = [
                    p for p in all_patterns
                    if p.confidence >= self._config.cross_skill_min_confidence
                ]
            else:
                logger.warning("Semantic store missing pattern retrieval methods")
                return []
            
            # Group by condition key
            for pattern in patterns:
                condition_key = self._condition_key(pattern.condition)
                
                group = pattern_groups[condition_key]
                group["condition_key"] = condition_key
                group["skills"].add(pattern.skill_name)
                group["patterns"].append(pattern)
            
            # Convert to list format and compute merged recommendations
            result = []
            for key, group in pattern_groups.items():
                result.append({
                    "condition_key": key,
                    "skills": list(group["skills"]),
                    "patterns": group["patterns"],
                    "common_recommendation": self._merge_recommendations(group["patterns"])
                })
            
            return result
            
        except Exception as e:
            logger.error(f"Error in _find_cross_skill_patterns: {e}", exc_info=True)
            return []
    
    def _condition_key(self, condition: Dict[str, Any]) -> str:
        """
        Create a hashable key from a condition dictionary.
        
        This normalizes conditions so similar conditions across skills
        can be grouped together.
        
        Args:
            condition: The condition dictionary from a SemanticPattern
        
        Returns:
            A stable hash string representing the condition
        """
        if not condition:
            return "empty"
        
        try:
            # Sort keys and create stable JSON representation
            normalized = json.dumps(condition, sort_keys=True, default=str)
            # Create a short hash for grouping
            hash_value = hashlib.md5(normalized.encode()).hexdigest()[:12]
            return hash_value
        except (TypeError, ValueError) as e:
            logger.warning(f"Failed to create condition key: {e}")
            return f"unknown_{id(condition)}"
    
    def _merge_recommendations(
        self,
        patterns: List[SemanticPattern]
    ) -> Dict[str, Any]:
        """
        Merge recommendations from multiple patterns.
        
        For numeric values, uses weighted average by confidence.
        For categorical values, uses mode (most common value).
        
        Args:
            patterns: List of patterns to merge recommendations from
        
        Returns:
            Merged recommendation dictionary
        """
        if not patterns:
            return {}
        
        # Collect all recommendation keys and their values
        key_values: Dict[str, List[tuple]] = defaultdict(list)
        
        for pattern in patterns:
            if not pattern.recommendation:
                continue
            
            for key, value in pattern.recommendation.items():
                # Store value with confidence weight
                key_values[key].append((value, pattern.confidence))
        
        merged = {}
        
        for key, value_pairs in key_values.items():
            if not value_pairs:
                continue
            
            values = [v for v, _ in value_pairs]
            weights = [w for _, w in value_pairs]
            
            # Determine type and merge accordingly
            sample_value = values[0]
            
            if isinstance(sample_value, (int, float)):
                # Weighted average for numeric values
                total_weight = sum(weights)
                if total_weight > 0:
                    weighted_sum = sum(v * w for v, w in value_pairs)
                    merged[key] = weighted_sum / total_weight
                else:
                    merged[key] = mean(values)
            elif isinstance(sample_value, bool):
                # Majority vote for boolean
                true_count = sum(1 for v in values if v)
                merged[key] = true_count > len(values) / 2
            elif isinstance(sample_value, str):
                # Mode for categorical/string values
                try:
                    merged[key] = mode(values)
                except StatisticsError:
                    # No unique mode, use first value
                    merged[key] = values[0]
            elif isinstance(sample_value, list):
                # Union for lists
                merged[key] = list(set(item for v in values for item in v))
            elif isinstance(sample_value, dict):
                # Recursive merge for nested dicts
                merged[key] = self._merge_recommendation_dicts(
                    [v for v in values if isinstance(v, dict)]
                )
            else:
                # Default: use most confident value
                max_weight_idx = weights.index(max(weights))
                merged[key] = values[max_weight_idx]
        
        return merged
    
    def _merge_recommendation_dicts(
        self,
        dicts: List[Dict[str, Any]]
    ) -> Dict[str, Any]:
        """
        Merge a list of dictionaries, handling overlapping keys.
        
        Args:
            dicts: List of dictionaries to merge
        
        Returns:
            Merged dictionary
        """
        if not dicts:
            return {}
        
        merged = {}
        all_keys = set()
        for d in dicts:
            all_keys.update(d.keys())
        
        for key in all_keys:
            values = [d[key] for d in dicts if key in d]
            if not values:
                continue
            
            sample = values[0]
            if isinstance(sample, (int, float)):
                merged[key] = mean(values)
            elif isinstance(sample, str):
                try:
                    merged[key] = mode(values)
                except StatisticsError:
                    merged[key] = values[0]
            else:
                merged[key] = values[0]
        
        return merged
    
    def _generate_description(self, group: Dict[str, Any]) -> str:
        """
        Generate a human-readable description for a procedural knowledge entry.
        
        Args:
            group: Pattern group with condition_key, skills, patterns, recommendation
        
        Returns:
            Human-readable description string
        """
        skills = group.get("skills", [])
        patterns = group.get("patterns", [])
        recommendation = group.get("common_recommendation", {})
        
        # Build description parts
        skill_count = len(skills)
        pattern_count = len(patterns)
        
        # Get domains involved
        domains = set()
        for pattern in patterns:
            if pattern.domain:
                domains.add(pattern.domain.value if hasattr(pattern.domain, 'value') else str(pattern.domain))
        
        domain_str = ", ".join(sorted(domains)) if domains else "generic"
        skill_str = ", ".join(sorted(skills)[:3])
        if len(skills) > 3:
            skill_str += f", and {len(skills) - 3} more"
        
        # Extract key recommendation elements
        rec_summary = []
        for key, value in list(recommendation.items())[:3]:
            if isinstance(value, float):
                rec_summary.append(f"{key}={value:.2f}")
            else:
                rec_summary.append(f"{key}={value}")
        rec_str = "; ".join(rec_summary) if rec_summary else "various parameters"
        
        description = (
            f"Cross-skill pattern observed across {skill_count} skills "
            f"({skill_str}) in {domain_str} domain(s). "
            f"Based on {pattern_count} semantic patterns, recommends: {rec_str}."
        )
        
        return description
    
    def _get_all_tenants(self) -> List[str]:
        """
        Get all tenant IDs from Firebase.
        
        Returns:
            List of tenant IDs
        """
        try:
            # Access Firebase through episodic store's client
            if hasattr(self._episodic, '_firebase'):
                firebase = self._episodic._firebase
                
                # Try to get tenant list from the episodic base path
                base_path = "system/intelligence/episodic"
                
                if hasattr(firebase, 'get_collection'):
                    # Get subcollections (tenants) under episodic
                    try:
                        result = firebase.get_collection(collection=base_path)
                        if result:
                            # Extract tenant IDs from document paths or IDs
                            tenants = []
                            for doc in result:
                                if isinstance(doc, dict):
                                    tenant_id = doc.get('_id') or doc.get('tenant_id')
                                    if tenant_id:
                                        tenants.append(tenant_id)
                            return tenants
                    except Exception as e:
                        logger.debug(f"Could not enumerate tenants: {e}")
                
                # Fallback: check if there's a tenant registry
                if hasattr(firebase, 'get_document'):
                    try:
                        registry = firebase.get_document(
                            collection="system/intelligence",
                            doc_id="tenant_registry"
                        )
                        if registry and 'tenants' in registry:
                            return registry['tenants']
                    except Exception:
                        pass
            
            logger.warning("Could not enumerate tenants, returning empty list")
            return []
            
        except Exception as e:
            logger.error(f"Error getting tenants: {e}")
            return []
    
    def _get_skills_for_tenant(self, tenant_id: str) -> List[str]:
        """
        Get all skill names for a tenant.
        
        Args:
            tenant_id: The tenant ID
        
        Returns:
            List of skill names
        """
        try:
            # Access Firebase through episodic store's client
            if hasattr(self._episodic, '_firebase'):
                firebase = self._episodic._firebase
                
                # Get skills for this tenant from episodic memory path
                tenant_path = f"system/intelligence/episodic/{tenant_id}"
                
                if hasattr(firebase, 'get_collection'):
                    try:
                        result = firebase.get_collection(collection=tenant_path)
                        if result:
                            skills = []
                            for doc in result:
                                if isinstance(doc, dict):
                                    skill_name = doc.get('_id') or doc.get('skill_name')
                                    if skill_name:
                                        skills.append(skill_name)
                            return skills
                    except Exception as e:
                        logger.debug(f"Could not enumerate skills for {tenant_id}: {e}")
                
                # Fallback: check semantic store for skill list
                if hasattr(self._semantic, 'get_skills_for_tenant'):
                    return self._semantic.get_skills_for_tenant(tenant_id)
            
            logger.warning(f"Could not enumerate skills for tenant {tenant_id}")
            return []

        except Exception as e:
            logger.error(f"Error getting skills for tenant {tenant_id}: {e}")
            return []

    def consolidate_from_module(
        self,
        module_id: str,
        client_id: str,
        execution_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        Extract learnings from a completed module and store in memory.

        This method is called when a module completes execution. It extracts
        patterns from the module's execution:
        - Successful skill sequences -> procedural memory
        - Client preferences -> semantic memory
        - Error patterns -> episodic memory

        Args:
            module_id: The completed module's ID
            client_id: The client/tenant ID
            execution_data: Optional execution state data containing:
                - skill_plan: List of skills executed
                - checkpoints: Execution checkpoints with status/outputs
                - outputs: Module outputs
                - error: Any error that occurred

        Returns:
            Dict with consolidation statistics:
            - episodes_stored: Number of episodic memories stored
            - patterns_updated: Number of semantic patterns updated
            - procedural_created: Number of procedural entries created
            - promotion_candidates: Number of patterns promoted
        """
        with self._lock:
            stats = {
                "episodes_stored": 0,
                "patterns_updated": 0,
                "procedural_created": 0,
                "promotion_candidates": 0,
            }

            if not execution_data:
                logger.debug(f"No execution data for module {module_id}")
                return stats

            try:
                logger.info(f"Consolidating memory from module {module_id} for client {client_id}")

                # Extract skill execution results from checkpoints
                checkpoints = execution_data.get("checkpoints", [])
                skill_plan = execution_data.get("skill_plan", [])
                module_outputs = execution_data.get("outputs", {})
                module_error = execution_data.get("error")

                # 1. Store execution summary in episodic memory
                episode = self._create_module_episode(
                    module_id=module_id,
                    client_id=client_id,
                    skill_plan=skill_plan,
                    checkpoints=checkpoints,
                    outputs=module_outputs,
                    error=module_error
                )

                if episode:
                    try:
                        # Note: store() extracts tenant_id/skill_name from episode.prediction
                        self._episodic.store(episode)
                        stats["episodes_stored"] += 1
                        logger.debug(f"Stored module episode {episode.episode_id}")
                    except Exception as e:
                        logger.warning(f"Failed to store module episode: {e}")

                # 2. Store individual skill results in episodic memory
                for checkpoint in checkpoints:
                    skill_episode = self._create_skill_episode(
                        module_id=module_id,
                        client_id=client_id,
                        checkpoint=checkpoint
                    )

                    if skill_episode:
                        try:
                            # Note: store() extracts tenant_id/skill_name from episode.prediction
                            self._episodic.store(skill_episode)
                            stats["episodes_stored"] += 1
                        except Exception as e:
                            logger.debug(f"Failed to store skill episode: {e}")

                # 3. Update semantic patterns from successful executions
                successful_skills = [
                    cp for cp in checkpoints
                    if cp.get("status") == "success"
                ]

                if successful_skills:
                    patterns_updated = self._update_patterns_from_execution(
                        client_id=client_id,
                        successful_skills=successful_skills,
                        module_outputs=module_outputs
                    )
                    stats["patterns_updated"] = patterns_updated

                # 4. Check for pattern promotion (episodic -> semantic when confidence > 0.7)
                promotion_result = self._check_for_promotions(client_id=client_id)
                stats["promotion_candidates"] = promotion_result.get("promoted", 0)

                # 5. If skill sequence was successful, consider for procedural memory
                if not module_error and len(skill_plan) >= 2:
                    procedural_count = self._evaluate_skill_sequence(
                        client_id=client_id,
                        skill_plan=skill_plan,
                        checkpoints=checkpoints
                    )
                    stats["procedural_created"] = procedural_count

                logger.info(f"Module {module_id} consolidation complete: {stats}")
                return stats

            except Exception as e:
                logger.error(f"Error consolidating from module {module_id}: {e}", exc_info=True)
                return stats

    def _create_module_episode(
        self,
        module_id: str,
        client_id: str,
        skill_plan: List[str],
        checkpoints: List[Dict[str, Any]],
        outputs: Dict[str, Any],
        error: Optional[str]
    ) -> Optional['EpisodicMemory']:
        """
        Create an episodic memory from module execution.

        Args:
            module_id: Module ID
            client_id: Client/tenant ID
            skill_plan: List of skills that were planned
            checkpoints: Execution checkpoints
            outputs: Module outputs
            error: Error message if failed

        Returns:
            EpisodicMemory or None
        """
        from ..types import EpisodicMemory, Prediction, Outcome, Domain
        import uuid

        try:
            # Calculate success metrics
            total_skills = len(skill_plan)
            successful_skills = sum(1 for cp in checkpoints if cp.get("status") == "success")
            success_rate = successful_skills / total_skills if total_skills > 0 else 0.0

            # Create prediction representing the module execution expectation
            prediction = Prediction(
                prediction_id=str(uuid.uuid4())[:12],
                skill_name="module_execution",
                tenant_id=client_id,
                domain=Domain.GENERIC,
                expected_signal=1.0,  # Expected all skills to succeed
                expected_baseline=1.0,
                confidence=0.5,
                context={
                    "module_id": module_id,
                    "skill_count": total_skills,
                    "skill_plan": skill_plan[:5],  # First 5 for context
                }
            )

            # Create outcome representing actual results
            outcome = Outcome(
                prediction_id=prediction.prediction_id,
                observed_signal=success_rate,
                observed_baseline=1.0,
                prediction_error=abs(1.0 - success_rate),
                goal_completed=(error is None and success_rate >= 0.8),
                business_impact=0.0,
                metadata={
                    "total_skills": total_skills,
                    "successful_skills": successful_skills,
                    "error": error,
                    "output_keys": list(outputs.keys()) if outputs else [],
                }
            )

            # Create episodic memory
            episode = EpisodicMemory(
                episode_id=f"mod_{module_id}_{str(uuid.uuid4())[:6]}",
                prediction=prediction,
                outcome=outcome,
                weight=1.0,  # Fresh memory, full weight
            )

            return episode

        except Exception as e:
            logger.error(f"Error creating module episode: {e}")
            return None

    def _create_skill_episode(
        self,
        module_id: str,
        client_id: str,
        checkpoint: Dict[str, Any]
    ) -> Optional['EpisodicMemory']:
        """
        Create an episodic memory from a skill checkpoint.

        Args:
            module_id: Module ID
            client_id: Client/tenant ID
            checkpoint: Skill checkpoint data

        Returns:
            EpisodicMemory or None
        """
        from ..types import EpisodicMemory, Prediction, Outcome, Domain
        import uuid

        try:
            skill_name = checkpoint.get("skill_name", "unknown")
            status = checkpoint.get("status", "unknown")
            output = checkpoint.get("output", {})
            error = checkpoint.get("error")

            # Create prediction
            prediction = Prediction(
                prediction_id=str(uuid.uuid4())[:12],
                skill_name=skill_name,
                tenant_id=client_id,
                domain=Domain.GENERIC,
                expected_signal=1.0,
                expected_baseline=1.0,
                confidence=0.5,
                context={
                    "module_id": module_id,
                    "skill_index": checkpoint.get("skill_index", 0),
                }
            )

            # Create outcome
            success = status == "success"
            outcome = Outcome(
                prediction_id=prediction.prediction_id,
                observed_signal=1.0 if success else 0.0,
                observed_baseline=1.0,
                prediction_error=0.0 if success else 1.0,
                goal_completed=success,
                business_impact=0.0,
                metadata={
                    "status": status,
                    "error": error,
                    "output_type": type(output).__name__ if output else None,
                }
            )

            # Create episode
            episode = EpisodicMemory(
                episode_id=f"skill_{skill_name}_{str(uuid.uuid4())[:6]}",
                prediction=prediction,
                outcome=outcome,
                weight=1.0,
            )

            return episode

        except Exception as e:
            logger.error(f"Error creating skill episode: {e}")
            return None

    def _update_patterns_from_execution(
        self,
        client_id: str,
        successful_skills: List[Dict[str, Any]],
        module_outputs: Dict[str, Any]
    ) -> int:
        """
        Update semantic patterns from successful skill executions.

        Args:
            client_id: Client/tenant ID
            successful_skills: List of successful skill checkpoints
            module_outputs: Module outputs

        Returns:
            Number of patterns updated
        """
        updated_count = 0

        for skill_data in successful_skills:
            skill_name = skill_data.get("skill_name", "")
            if not skill_name:
                continue

            try:
                # Check if semantic store has update method
                if hasattr(self._semantic, 'update_from_outcome'):
                    # Update any matching patterns for this skill
                    # Use GENERIC domain as fallback
                    self._semantic.update_from_outcome(
                        pattern_id=f"{skill_name}_default",
                        domain=Domain.GENERIC,
                        success=True,
                        observed_ratio=1.0
                    )
                    updated_count += 1
            except Exception as e:
                logger.debug(f"Could not update pattern for {skill_name}: {e}")

        return updated_count

    def _check_for_promotions(self, client_id: str) -> Dict[str, int]:
        """
        Check for patterns that should be promoted (confidence > 0.7).

        Episodic memories with high confidence/consistency should be
        promoted to semantic patterns.

        Args:
            client_id: Client/tenant ID

        Returns:
            Dict with promotion statistics
        """
        stats = {"promoted": 0, "candidates": 0}

        try:
            # Get recent episodic memories for this client
            if hasattr(self._episodic, 'retrieve'):
                # Get skills that might have consolidation candidates
                skills = self._get_skills_for_tenant(client_id)

                for skill_name in skills[:10]:  # Limit to avoid too many queries
                    episodes = self._episodic.retrieve(
                        tenant_id=client_id,
                        skill_name=skill_name,
                        min_weight=0.3,  # Not too old
                        limit=20
                    )

                    if len(episodes) >= self._config.min_episodes_for_consolidation:
                        stats["candidates"] += 1

                        # Calculate average success rate
                        successes = sum(1 for e in episodes if e.outcome.goal_completed)
                        success_rate = successes / len(episodes)

                        # Promote if high confidence
                        if success_rate >= 0.7:
                            if hasattr(self._semantic, 'consolidate_from_episodes'):
                                result = self._semantic.consolidate_from_episodes(episodes)
                                if result:
                                    stats["promoted"] += 1
                                    logger.info(
                                        f"Promoted pattern for {skill_name} with "
                                        f"{success_rate:.0%} success rate"
                                    )

        except Exception as e:
            logger.debug(f"Error checking for promotions: {e}")

        return stats

    def _evaluate_skill_sequence(
        self,
        client_id: str,
        skill_plan: List[str],
        checkpoints: List[Dict[str, Any]]
    ) -> int:
        """
        Evaluate a skill sequence for procedural knowledge creation.

        Args:
            client_id: Client/tenant ID
            skill_plan: List of skills executed
            checkpoints: Execution checkpoints

        Returns:
            Number of procedural entries created
        """
        created_count = 0

        try:
            # Only consider if all skills succeeded
            all_success = all(
                cp.get("status") == "success"
                for cp in checkpoints
            )

            if not all_success or len(skill_plan) < 2:
                return 0

            # Check if we have enough evidence across multiple executions
            # (this would require tracking sequences over time)
            # For now, just log that this sequence succeeded
            logger.debug(
                f"Successful skill sequence for {client_id}: {' -> '.join(skill_plan)}"
            )

            # In a full implementation, we would:
            # 1. Track this sequence in a sequence registry
            # 2. Count how many times it succeeded across executions
            # 3. Create procedural knowledge when threshold met

        except Exception as e:
            logger.debug(f"Error evaluating skill sequence: {e}")

        return created_count


__all__ = [
    "ConsolidationConfig",
    "MemoryConsolidationManager",
]
